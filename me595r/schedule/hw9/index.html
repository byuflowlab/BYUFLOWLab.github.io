<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      HW &middot; ME 595R
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/my.css">
  <link rel="stylesheet" href="/public/css/academicons.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/public/css/toc.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <!-- Icons -->
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png"> -->
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/public/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/public/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/public/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/public/apple-touch-icon-60x60.png" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/public/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/public/apple-touch-icon-76x76.png" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/public/apple-touch-icon-152x152.png" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- script -->
  <script type="text/javascript">  // used to hide/show BibTeX blocks
  <!--
      function toggle_visibility(id) {
         var e = document.getElementById(id);
         if(e.style.display == 'block')
            e.style.display = 'none';
         else
            e.style.display = 'block';
      }
  //-->
  </script>

  <!-- mathjax -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>


</head>


  <body class="theme-base-08 layout-top">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/me595r">
          ME 595R
        </a>
      </h1>
      <p class="lead">Deep Learning for Engineers</p>
    </div>

    <nav class="sidebar-nav">

      <a class="sidebar-nav-item" href="/me595r/syllabus">Syllabus</a>
      <a class="sidebar-nav-item" href="/me595r/schedule">Schedule</a>
      <a class="sidebar-nav-item" href="/me595r/resources">Resources</a>

    </nav>

    <p>&copy; 2025. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
    <!-- <h1 class="page-title">HW</h1> -->
      <h2 id="hw-9-graph-neural-network-with-newtonian-physics">HW 9: Graph neural network with Newtonian Physics</h2>

<p>due 3/20/2025 before midnight via Learning Suite
<span style="float:right;">25 possible points</span></p>

<hr />

<p>We will reproduce a portion of <a href="https://arxiv.org/abs/2006.11287">this paper</a>.  You will need to install <a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a>.  There is also a <a href="https://github.com/MilesCranmer/symbolic_deep_learning/tree/master">code repository</a> and a demo that same link (the relevant files for you are models.py and the demo).  You are welcome to refer to these, but should treat it like output from an AI chatbot (i.e., don’t blindly copy/paste, but use to develop understanding).  We are only doing a subset of the paper so don’t get lost in the broader details of this code repo, mainly it will help you will some hyperparameters and GNN structure.</p>

<p>This homework focuses on the 2D spring case.  We will not do the symbolic regression part (though I encourage you to read through that), and will just focus on the graph neural network training.
I’ve pulled out data in the following <a href="../spring_data.npz">npz file</a> and started a script to process this data and provided additional guidance in the comments of the following <a href="../hw9setup.py">python script</a>.  In the file I pulled out one set of training trajectories <code class="language-plaintext highlighter-rouge">train_traj</code> and one set of testing trajectories <code class="language-plaintext highlighter-rouge">train_traj</code>.  The goal is to reasonably reproduce the testing trajectories with your graph neural net (the training trajectory is just provided as an additional guide since it should be a little easier to reproduce as its data will be in the training set).  You will need to use <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html">scipy.integrate.solve_ivp</a> to propagate the accelerations predicted by your GNN to compare the true and predicted trajectories (the time span ranges from 0 to 5).  There is no need to animate or use different transparencies to indicate time.  Just plot the full path like I do in my example.  Make sure that you use the same colors for each particle in true vs. predicted, or put each particle on a separate plot, so we can clearly see the comparison between true and predicted.</p>

<p>Tips:</p>
<ul>
  <li>In the demo they use a large message dimension (which corresponds to forces) and then add on an extra L1 loss function to encourage this dimension to be small.  I just enforced the dimension to be small from the beginning (2 since that is what we know the dimension should be ideally). This is simpler and avoids the need for another loss function.  Note that this works well for this scenario, but the opposite approach of allowing the network the extra freedom often allows better solutions (“L1” vs “bottleneck” in the paper).</li>
  <li>If you use a small message dimension like I did, you also won’t need your hidden dimension to be nearly so large.</li>
  <li>I only generated 1/10th of the data they did (and you may not even need all of that).  I also did not use any data augmentation as they did in the paper.  If we were doing symbolic regression then really tightening this up with more data would be justified, but for the purposes of this homework where we are just learning about GNNs the accuracy of our predictions will be plenty good.</li>
  <li>Note that they used MAE rather than MSE for their loss function.</li>
  <li>I didn’t need a learning rate scheduler, but YMMV.</li>
</ul>

    </div>

  </body>
</html>
