<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      HW &middot; ME 595R
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/my.css">
  <link rel="stylesheet" href="/public/css/academicons.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/public/css/toc.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <!-- Icons -->
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png"> -->
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/public/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/public/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/public/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/public/apple-touch-icon-60x60.png" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/public/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/public/apple-touch-icon-76x76.png" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/public/apple-touch-icon-152x152.png" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- script -->
  <script type="text/javascript">  // used to hide/show BibTeX blocks
  <!--
      function toggle_visibility(id) {
         var e = document.getElementById(id);
         if(e.style.display == 'block')
            e.style.display = 'none';
         else
            e.style.display = 'block';
      }
  //-->
  </script>

  <!-- mathjax -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>


</head>


  <body class="theme-base-08 layout-top">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/me595r">
          ME 595R
        </a>
      </h1>
      <p class="lead">Deep Learning for Engineers</p>
    </div>

    <nav class="sidebar-nav">

      <a class="sidebar-nav-item" href="/me595r/syllabus">Syllabus</a>
      <a class="sidebar-nav-item" href="/me595r/schedule">Schedule</a>
      <a class="sidebar-nav-item" href="/me595r/resources">Resources</a>

    </nav>

    <p>&copy; 2026. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
    <!-- <h1 class="page-title">HW</h1> -->
      <h2 id="hw-6-neural-ode-with-autoencoder-and-physics-loss">HW 6: Neural ODE with Autoencoder (and physics loss)</h2>

<p>due 2/20/2025 before midnight via Learning Suite
<span style="float:right;">25 possible points</span></p>

<hr />

<p>Autoencoders are used in many architectures to either reduce or increase dimensionality.  In this problem we solve a high dimensional ODE problem by first projecting it into a low dimensional space (encoder).  This low dimensional space is called the latent space.  Next we solve the neural ODE in the latent space.  Finally, we project the solution back to the original high-dimensional space (decoder).  Our objective will simultaneously train the neural net in the ODE and the neural nets for the encoder/decoder.</p>

<p><a href="https://www.nature.com/articles/s41598-023-36799-6">This paper</a> uses that approach but also adds a physics-based loss term.  That allows us to have more accurate solutions outside the training data.  Read the methods section to better understand the formulation.</p>

<p>We’ll reproduce the first example: the lifted duffing oscillator.  In <a href="../pinodedata.py">this file</a> I’ve written functions that generate training data, testing data, and collocation points.  I’ve reproduced the results in the paper, but it takes a little while to run.  To make things easier I’ve expanded the range of the training data a bit (rather than limiting to just the red region, I expanded the training data to a little over half the domain with collocation points on the remainder of the domain).  That should allow you to get by with significantly less training data and collocation points (I used about 600 training points, and 10,000 collocation points but could have gotten away with less).  Even still, you may find it beneficial to use a GPU. Our goal is to get the MSE error for the test set (100 test points) below 0.1.</p>

<p>You should also be able to produce a plot like the lower right of Fig 3 (except we won’t worry about coloring separate regions).  I provided a function true_encoder that you can use (the paper also uses the true encoder for the visualization).  We can use our trained encoder for this projection, but it won’t necessarily match since there will be many possible solutions for a good latent space.  So in general this isn’t something that one would know, it just helps in this particularly case where we know what the projection looks like to see if our training is on track.</p>

<p>Tips:</p>

<ul>
  <li>torchdiffeq uses float64 by default, and you’ll probably want to keep it that way, especially if using a discrete adjoint.  But if you want to use float32 you can change it by passing <code class="language-plaintext highlighter-rouge">options={'dtype': torch.float32}</code> to odeint.</li>
  <li>Figure 1 seems to suggest that the prediction loss only compares the final time step, but you’ll have the full trajectory and should compare all of it (which is what they actually do as shown in Eq 5).  Similarly, that figure seems to suggest that the reconstruction loss uses only the initial time step, but like Eq 4 shows, you’d want to check reconstruction for the full trajectory.</li>
  <li>Make sure you understand how <code class="language-plaintext highlighter-rouge">reshape</code> works and make sure you aren’t using it when you should be using <code class="language-plaintext highlighter-rouge">permute</code>.</li>
  <li>If you want to batch, you’ll need to create two data loaders since the training data and the collocation data have different sizes.  To sync them up you could preselect the number of batches you want (<code class="language-plaintext highlighter-rouge">nbatches</code> below) then use that number to calculate what batch size you need for each.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">batch_size_train</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">ntrain</span> <span class="o">/</span> <span class="n">nbatches</span><span class="p">)</span>
  <span class="n">batch_size_collocation</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">ncol</span> <span class="o">/</span> <span class="n">nbatches</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>You can get dz/dx in a similar manner to what we’ve done on previous homeworks with autograd.grad (you’ll need two separate calls to autograd.grad since you have two different batched outputs for the two dimensions of z).  You’ll then need to use torch.matmul or torch.bmm to get to dz/dt.  There is a Jacobian function that can be used instead of two calls to grad, and there is also a jvp function (Jacobian vector product) that could be used to combine the grad and matrix multiply steps in one line.  But understanding these in a batched mode takes more explanation so I’d just use autograd.grad, and if you’re interested in the other options we can discuss on Slack.</li>
  <li>Per usual, start small. Get things running with a small number of collocation points, and perhaps weights.</li>
  <li>If you want to use a GPU, Google Colab is a good option.  In the upper right of Colab you need to change your runtime type to the GPU.  In your code you need to set <code class="language-plaintext highlighter-rouge">device = "cuda"</code> (to allow it to run on the GPU or locally without changing code you can use <code class="language-plaintext highlighter-rouge">device = 'cuda' if torch.cuda.is_available() else 'cpu'</code>).  You also need to moved all the data in your torch tensors, including those in the model, to the gpu device.  (i.e., <code class="language-plaintext highlighter-rouge">model.double().to(device)</code>, <code class="language-plaintext highlighter-rouge">torch.tensor(x, dtype=torch.float64, device=device)</code>).  Note that Google has time limits on free GPU usage.</li>
  <li>In their <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-023-36799-6/MediaObjects/41598_2023_36799_MOESM1_ESM.pdf">supplementary document</a> they note some of the hyperparameters for their networks, which will save you some time.</li>
</ul>

    </div>

  </body>
</html>
