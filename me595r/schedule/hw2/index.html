<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      HW &middot; ME 595R
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/my.css">
  <link rel="stylesheet" href="/public/css/academicons.css">
  <link rel="stylesheet" type="text/css" media="screen" href="/public/css/toc.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <!-- Icons -->
  <!-- <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png"> -->
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/public/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/public/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/public/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/public/apple-touch-icon-60x60.png" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/public/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/public/apple-touch-icon-76x76.png" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/public/apple-touch-icon-152x152.png" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- script -->
  <script type="text/javascript">  // used to hide/show BibTeX blocks
  <!--
      function toggle_visibility(id) {
         var e = document.getElementById(id);
         if(e.style.display == 'block')
            e.style.display = 'none';
         else
            e.style.display = 'block';
      }
  //-->
  </script>

  <!-- mathjax -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>


</head>


  <body class="theme-base-08 layout-top">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/me595r">
          ME 595R
        </a>
      </h1>
      <p class="lead">Deep Learning for Engineers</p>
    </div>

    <nav class="sidebar-nav">

      <a class="sidebar-nav-item" href="/me595r/syllabus">Syllabus</a>
      <a class="sidebar-nav-item" href="/me595r/schedule">Schedule</a>
      <a class="sidebar-nav-item" href="/me595r/resources">Resources</a>

    </nav>

    <p>&copy; 2026. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
    <!-- <h1 class="page-title">HW</h1> -->
      <h2 id="hw-2-write-your-own">HW 2: Write Your Own</h2>

<hr />

<p>We will use the same dataset as the last homework, but will now write our own (basic) neural net so that we can better understand what’s happening under the hood. I created a <a href="../template.py">template file</a> to help you get started, but you’re welcome to discard it and organize it your own way if you prefer.  Either way, you’ll need to write your own functions or classes for activation, initialization, loss, layers, network, and optimization. For the activation, loss, layers, and networks, you’ll also need to write the backpropagation routines. I’ve done all the data preparation for you (in the template file).  It’s the same stuff we did last time only done manually since we’re just using numpy and not torch (and we’re skipping batching since it’s not really needed on this small dataset). For the optimizer we will just use plain gradient descent.  It will work fine in this case, but you’ll likely need to use a far smaller learning rate and will correspondingly need to increase the number of epochs.</p>

<h4 id="debugging">Debugging</h4>

<p>This homework can be tricky so I’ve provided an example below to help you debug.  This example does not represent realistic data or a reasonably sized network, but is just a small example to facilitate checking that your calculations.  I set <code class="language-plaintext highlighter-rouge">np.random.seed(0)</code> to aid in reproducibility, but just to be sure, I’ve also printed the weights and biases below.  In this example I created 2 layers. The first one goes from 2 nodes to 3 nodes.  The second one from 3 nodes down to 1 node.
Initial weights and biases:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l1</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.34710014</span> <span class="mf">0.45232547</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.38122103</span> <span class="mf">0.34461438</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.26794282</span> <span class="mf">0.4084993</span> <span class="p">]]</span>
<span class="n">l1</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span><span class="p">]]</span>
<span class="n">l2</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.30942088</span> <span class="mf">0.63057874</span> <span class="mf">0.68141247</span><span class="p">]]</span>
<span class="n">l2</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.</span><span class="p">]]</span>
</code></pre></div></div>

<p>I initialized my first layer with 2 inputs and 10 data points all set to one.  I also set my outputs to ones.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>
</code></pre></div></div>
<p>For those weights and “data”, I get the following MSE loss: <code class="language-plaintext highlighter-rouge">0.02755316457552718</code> with <code class="language-plaintext highlighter-rouge">yhat = array([[1.16599146, ...</code> (repeated 10 times since I made all the data the same)</p>

<p>My derivatives are as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Wbar</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.10272245</span> <span class="mf">0.10272245</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.20934137</span> <span class="mf">0.20934137</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.2262173</span>  <span class="mf">0.2262173</span> <span class="p">]]</span>
<span class="n">bbar</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.10272245</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.20934137</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.2262173</span> <span class="p">]]</span>
<span class="n">Wbar</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.26539565</span> <span class="mf">0.24096496</span> <span class="mf">0.22456723</span><span class="p">]]</span>
<span class="n">bbar</span> <span class="o">=</span>  <span class="p">[[</span><span class="mf">0.33198292</span><span class="p">]]</span>
</code></pre></div></div>

<p>just for completness, the derivative of the last Zbar before the loss function (or we could call it Xbar if you consider the identity function as the last activation) is <code class="language-plaintext highlighter-rouge">0.03319829</code> repeated 10 times in an array.</p>

    </div>

  </body>
</html>
